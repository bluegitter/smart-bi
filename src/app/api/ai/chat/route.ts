import { NextRequest, NextResponse } from 'next/server'
import { requireAuth } from '@/lib/middleware/auth'
import { LLMConfig } from '@/models/LLMConfig'
import { connectDB } from '@/lib/mongodb'
import { z } from 'zod'

// AIèŠå¤©è¯·æ±‚éªŒè¯æ¨¡å¼
const chatRequestSchema = z.object({
  message: z.string().min(1, 'æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º').max(2000, 'æ¶ˆæ¯å†…å®¹ä¸èƒ½è¶…è¿‡2000å­—ç¬¦'),
  history: z.array(z.object({
    id: z.string(),
    role: z.enum(['user', 'assistant']),
    content: z.string(),
    timestamp: z.date().or(z.string())
  })).optional().default([])
})

interface Message {
  role: 'user' | 'assistant'
  content: string
}

// POST /api/ai/chat - AIèŠå¤©å¯¹è¯
export async function POST(request: NextRequest) {
  try {
    const { user, error } = await requireAuth(request)
    if (error) {
      return NextResponse.json(error, { status: error.status })
    }

    await connectDB()

    const body = await request.json()
    
    // éªŒè¯è¯·æ±‚æ•°æ®
    const validationResult = chatRequestSchema.safeParse(body)
    if (!validationResult.success) {
      return NextResponse.json(
        { 
          error: 'è¯·æ±‚æ•°æ®æ— æ•ˆ',
          details: validationResult.error.errors
        },
        { status: 400 }
      )
    }

    const { message, history } = validationResult.data

    console.log(`ğŸ¤– [AI Chat] ç”¨æˆ· ${user.name} å‘èµ·AIå¯¹è¯:`)
    console.log(`   ğŸ“ æ¶ˆæ¯: ${message.substring(0, 100)}${message.length > 100 ? '...' : ''}`)
    console.log(`   ğŸ“š å†å²è®°å½•: ${history.length} æ¡æ¶ˆæ¯`)

    // è·å–ç”¨æˆ·çš„é»˜è®¤LLMé…ç½®
    const defaultConfig = await LLMConfig.findOne({
      userId: user._id,
      isDefault: true,
      isActive: true
    }).select('+config.apiKey')

    if (!defaultConfig) {
      console.log(`âŒ [AI Chat] ç”¨æˆ· ${user._id} æœªé…ç½®é»˜è®¤LLM`)
      return NextResponse.json(
        { error: 'æœªæ‰¾åˆ°å¯ç”¨çš„AIæ¨¡å‹é…ç½®ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½®LLMæœåŠ¡' },
        { status: 400 }
      )
    }

    console.log(`ğŸ”§ [AI Chat] ä½¿ç”¨é…ç½®: ${defaultConfig.displayName} (${defaultConfig.provider}/${defaultConfig.config.model})`)

    // æ„å»ºå¯¹è¯æ¶ˆæ¯
    const messages: Message[] = []
    
    // æ·»åŠ ç³»ç»Ÿæç¤º
    messages.push({
      role: 'assistant' as const,
      content: 'ä½ æ˜¯Smart BIç³»ç»Ÿçš„AIæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·è§£ç­”å…³äºæ•°æ®åˆ†æã€å•†ä¸šæ™ºèƒ½ã€æ•°æ®å¯è§†åŒ–ç­‰ç›¸å…³é—®é¢˜ã€‚è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚'
    })

    // æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘10æ¡ï¼‰
    const recentHistory = history.slice(-10)
    recentHistory.forEach(msg => {
      messages.push({
        role: msg.role,
        content: msg.content
      })
    })

    // æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.push({
      role: 'user',
      content: message
    })

    const startTime = Date.now()

    try {
      // è°ƒç”¨LLM API
      const aiResponse = await callLLMAPI(defaultConfig, messages)
      const responseTime = Date.now() - startTime
      
      console.log(`âœ… [AI Chat] AIå“åº”æˆåŠŸ:`)
      console.log(`   â±ï¸  å“åº”æ—¶é—´: ${responseTime}ms`)
      console.log(`   ğŸ“„ å“åº”é•¿åº¦: ${aiResponse.length} å­—ç¬¦`)
      
      return NextResponse.json({
        message: aiResponse,
        model: defaultConfig.config.model,
        provider: defaultConfig.provider,
        responseTime
      })
    } catch (llmError) {
      const errorMsg = llmError instanceof Error ? llmError.message : 'AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨'
      console.log(`âŒ [AI Chat] LLMè°ƒç”¨å¤±è´¥:`)
      console.log(`   â±ï¸  å“åº”æ—¶é—´: ${Date.now() - startTime}ms`)
      console.log(`   ğŸ”¥ é”™è¯¯ä¿¡æ¯: ${errorMsg}`)
      
      return NextResponse.json(
        { error: `AIæœåŠ¡å¼‚å¸¸: ${errorMsg}` },
        { status: 503 }
      )
    }

  } catch (error) {
    console.error('AIèŠå¤©æœåŠ¡å¤±è´¥:', error)
    return NextResponse.json(
      { error: 'AIèŠå¤©æœåŠ¡å¼‚å¸¸' },
      { status: 500 }
    )
  }
}

// è°ƒç”¨LLM APIçš„å…·ä½“å®ç°
async function callLLMAPI(config: any, messages: Message[]): Promise<string> {
  const { provider, config: llmConfig } = config
  const { apiKey, apiUrl, model } = llmConfig

  let requestUrl: string
  let requestHeaders: Record<string, string>
  let requestBody: any

  // æ ¹æ®ä¸åŒæä¾›å•†æ„å»ºè¯·æ±‚
  switch (provider) {
    case 'openai':
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.openai.com/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7,
        top_p: llmConfig.topP,
        frequency_penalty: llmConfig.frequencyPenalty,
        presence_penalty: llmConfig.presencePenalty
      }
      break

    case 'anthropic':
      requestUrl = apiUrl || 'https://api.anthropic.com/v1/messages'
      requestHeaders = {
        'x-api-key': apiKey,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01'
      }
      // Anthropicéœ€è¦ç‰¹æ®Šçš„æ¶ˆæ¯æ ¼å¼
      const anthropicMessages = messages.filter(m => m.role !== 'assistant' || messages.indexOf(m) > 0)
      requestBody = {
        model,
        messages: anthropicMessages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7
      }
      break

    case 'zhipu':
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://open.bigmodel.cn/api/paas/v4/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7
      }
      break

    case 'moonshot':
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.moonshot.cn/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7
      }
      break

    case 'deepseek':
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.deepseek.com/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7
      }
      break

    case 'custom':
      if (!apiUrl) {
        throw new Error('è‡ªå®šä¹‰æä¾›å•†éœ€è¦æŒ‡å®šAPI URL')
      }
      requestUrl = apiUrl.endsWith('/chat/completions') 
        ? apiUrl 
        : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages,
        max_tokens: llmConfig.maxTokens || 1000,
        temperature: llmConfig.temperature || 0.7
      }
      break

    default:
      throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›å•†: ${provider}`)
  }

  // å‘é€è¯·æ±‚åˆ°LLM API
  const response = await fetch(requestUrl, {
    method: 'POST',
    headers: requestHeaders,
    body: JSON.stringify(requestBody),
    signal: AbortSignal.timeout(60000) // 60ç§’è¶…æ—¶
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`LLM APIè¯·æ±‚å¤±è´¥ (${response.status}): ${errorText}`)
  }

  const result = await response.json()

  // è§£æå“åº”å†…å®¹
  if (provider === 'anthropic') {
    // Anthropicå“åº”æ ¼å¼
    if (result.content && result.content[0] && result.content[0].text) {
      return result.content[0].text
    } else {
      throw new Error('Anthropic APIå“åº”æ ¼å¼å¼‚å¸¸')
    }
  } else {
    // OpenAIå…¼å®¹æ ¼å¼
    if (result.choices && result.choices[0] && result.choices[0].message) {
      return result.choices[0].message.content
    } else {
      throw new Error('LLM APIå“åº”æ ¼å¼å¼‚å¸¸')
    }
  }
}