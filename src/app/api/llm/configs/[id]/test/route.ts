import { NextRequest, NextResponse } from 'next/server'
import { requireAuth } from '@/lib/middleware/auth'
import { LLMConfig } from '@/models/LLMConfig'
import { connectDB } from '@/lib/mongodb'
import type { LLMTestResult } from '@/types/llm'

// POST /api/llm/configs/[id]/test - æµ‹è¯•LLMé…ç½®è¿æ¥
export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  try {
    const { user, error } = await requireAuth(request)
    if (error) {
      return NextResponse.json(error, { status: error.status })
    }

    await connectDB()

    // ç­‰å¾…å¹¶è·å–å‚æ•°
    const { id } = await params

    // æŸ¥æ‰¾é…ç½®
    const config = await LLMConfig.findById(id).select('+config.apiKey')
    
    if (!config) {
      console.log(`âŒ [LLM Test] é…ç½®ä¸å­˜åœ¨: ${id}`)
      return NextResponse.json(
        { error: 'é…ç½®ä¸å­˜åœ¨' },
        { status: 404 }
      )
    }

    // æ£€æŸ¥æƒé™
    if (config.userId.toString() !== user._id.toString()) {
      console.log(`âŒ [LLM Test] ç”¨æˆ· ${user._id} æ— æƒæµ‹è¯•é…ç½® ${id}`)
      return NextResponse.json(
        { error: 'æ— æƒæµ‹è¯•æ­¤é…ç½®' },
        { status: 403 }
      )
    }

    console.log(`ğŸ§ª [LLM Test] å¼€å§‹æµ‹è¯•è¿æ¥:`)
    console.log(`   ğŸ“‹ é…ç½®ID: ${id}`)
    console.log(`   ğŸ‘¤ ç”¨æˆ·: ${user.name} (${user.email})`)
    console.log(`   ğŸ·ï¸  é…ç½®åç§°: ${config.displayName}`)
    console.log(`   ğŸ”§ æä¾›å•†: ${config.provider}`)
    console.log(`   ğŸ¤– æ¨¡å‹: ${config.config.model}`)
    console.log(`   ğŸŒ APIåœ°å€: ${config.config.apiUrl || 'ä½¿ç”¨é»˜è®¤åœ°å€'}`)

    const startTime = Date.now()
    let testResult: LLMTestResult

    try {
      // æ ¹æ®ä¸åŒæä¾›å•†æ‰§è¡Œæµ‹è¯•
      testResult = await testLLMConnection(config)
      testResult.responseTime = Date.now() - startTime
      
      console.log(`âœ… [LLM Test] è¿æ¥æµ‹è¯•æˆåŠŸ:`)
      console.log(`   â±ï¸  å“åº”æ—¶é—´: ${testResult.responseTime}ms`)
      console.log(`   ğŸ“Š Tokenä½¿ç”¨: prompt=${testResult.usage?.promptTokens}, completion=${testResult.usage?.completionTokens}, total=${testResult.usage?.totalTokens}`)
      console.log(`   ğŸ“ˆ æ¨¡å‹ä¿¡æ¯: ${testResult.modelInfo?.model}`)
    } catch (testError) {
      const errorMsg = testError instanceof Error ? testError.message : 'è¿æ¥æµ‹è¯•å¤±è´¥'
      console.log(`âŒ [LLM Test] è¿æ¥æµ‹è¯•å¤±è´¥:`)
      console.log(`   â±ï¸  å“åº”æ—¶é—´: ${Date.now() - startTime}ms`)
      console.log(`   ğŸ”¥ é”™è¯¯ä¿¡æ¯: ${errorMsg}`)
      
      testResult = {
        success: false,
        responseTime: Date.now() - startTime,
        error: errorMsg
      }
    }

    return NextResponse.json(testResult)
  } catch (error) {
    console.error('LLMé…ç½®æµ‹è¯•å¤±è´¥:', error)
    return NextResponse.json(
      { error: 'æµ‹è¯•è¿æ¥å¤±è´¥' },
      { status: 500 }
    )
  }
}

// æµ‹è¯•LLMè¿æ¥çš„å…·ä½“å®ç°
async function testLLMConnection(config: any): Promise<LLMTestResult> {
  const { provider, config: llmConfig } = config
  const { apiKey, apiUrl, model } = llmConfig

  // æ„å»ºæµ‹è¯•æ¶ˆæ¯
  const testMessages = [
    {
      role: 'user' as const,
      content: 'ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿æ¥æµ‹è¯•ã€‚è¯·ç®€å•å›å¤"è¿æ¥æˆåŠŸ"ã€‚'
    }
  ]

  let requestUrl: string
  let requestHeaders: Record<string, string>
  let requestBody: any

  // æ ¹æ®ä¸åŒæä¾›å•†æ„å»ºè¯·æ±‚
  switch (provider) {
    case 'openai':
      // å¦‚æœæä¾›äº†è‡ªå®šä¹‰API URLï¼Œéœ€è¦ç¡®ä¿åŒ…å«å®Œæ•´çš„ç«¯ç‚¹è·¯å¾„
      if (apiUrl) {
        // å¦‚æœè‡ªå®šä¹‰URLå·²ç»åŒ…å«äº†chat/completionsç«¯ç‚¹ï¼Œç›´æ¥ä½¿ç”¨
        // å¦åˆ™æ·»åŠ chat/completionsç«¯ç‚¹
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.openai.com/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50,
        temperature: 0.1
      }
      break

    case 'anthropic':
      requestUrl = apiUrl || 'https://api.anthropic.com/v1/messages'
      requestHeaders = {
        'x-api-key': apiKey,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50
      }
      break

    case 'zhipu':
      // æ™ºè°±AI OpenAIå…¼å®¹æ¥å£
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://open.bigmodel.cn/api/paas/v4/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50,
        temperature: 0.1
      }
      break

    case 'moonshot':
      // Moonshot OpenAIå…¼å®¹æ¥å£
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.moonshot.cn/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50,
        temperature: 0.1
      }
      break

    case 'deepseek':
      // DeepSeek OpenAIå…¼å®¹æ¥å£
      if (apiUrl) {
        requestUrl = apiUrl.endsWith('/chat/completions') 
          ? apiUrl 
          : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      } else {
        requestUrl = 'https://api.deepseek.com/v1/chat/completions'
      }
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50,
        temperature: 0.1
      }
      break

    case 'custom':
      if (!apiUrl) {
        throw new Error('è‡ªå®šä¹‰æä¾›å•†éœ€è¦æŒ‡å®šAPI URL')
      }
      // è‡ªå®šä¹‰æä¾›å•†ï¼Œæ™ºèƒ½å¤„ç†API URL
      requestUrl = apiUrl.endsWith('/chat/completions') 
        ? apiUrl 
        : `${apiUrl.replace(/\/$/, '')}/chat/completions`
      requestHeaders = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      }
      requestBody = {
        model,
        messages: testMessages,
        max_tokens: 50,
        temperature: 0.1
      }
      break

    default:
      throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›å•†: ${provider}`)
  }

  // å‘é€æµ‹è¯•è¯·æ±‚
  const response = await fetch(requestUrl, {
    method: 'POST',
    headers: requestHeaders,
    body: JSON.stringify(requestBody),
    signal: AbortSignal.timeout(30000) // 30ç§’è¶…æ—¶
  })

  if (!response.ok) {
    const errorText = await response.text()
    throw new Error(`APIè¯·æ±‚å¤±è´¥ (${response.status}): ${errorText}`)
  }

  const result = await response.json()

  // è§£æå“åº”
  let usage
  let modelInfo

  if (provider === 'anthropic') {
    // Anthropicå“åº”æ ¼å¼
    usage = {
      promptTokens: result.usage?.input_tokens || 0,
      completionTokens: result.usage?.output_tokens || 0,
      totalTokens: (result.usage?.input_tokens || 0) + (result.usage?.output_tokens || 0)
    }
    modelInfo = {
      model: result.model || model,
      capabilities: ['chat', 'completion']
    }
  } else {
    // OpenAIå…¼å®¹æ ¼å¼
    usage = {
      promptTokens: result.usage?.prompt_tokens || 0,
      completionTokens: result.usage?.completion_tokens || 0,
      totalTokens: result.usage?.total_tokens || 0
    }
    modelInfo = {
      model: result.model || model,
      capabilities: ['chat', 'completion']
    }
  }

  return {
    success: true,
    responseTime: 0, // å°†åœ¨è°ƒç”¨å¤„è®¾ç½®
    modelInfo,
    usage
  }
}