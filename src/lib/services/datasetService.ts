import { Dataset } from '@/models/Dataset'
import { DataSource } from '@/models/DataSource'
import { connectDB } from '@/lib/mongodb'
import { executeQuery } from '@/lib/mysql'
import { datasetCache, queryCache, CacheKeys } from '@/lib/cache/CacheManager'
import type { 
  Dataset as DatasetType, 
  DatasetField, 
  DatasetPreview, 
  DatasetStats,
  FieldStats,
  CreateDatasetRequest,
  UpdateDatasetRequest,
  DatasetSearchParams,
  DataQualityIssue
} from '@/types/dataset'

export class DatasetService {
  // åˆ›å»ºæ•°æ®é›†
  static async createDataset(userId: string, request: CreateDatasetRequest): Promise<DatasetType> {
    await connectDB()
    
    // éªŒè¯æ•°æ®æº
    let datasourceId: string = ''
    if (request.tableConfig) {
      datasourceId = request.tableConfig.datasourceId
    } else if (request.sqlConfig) {
      datasourceId = request.sqlConfig.datasourceId
    }
    
    if (datasourceId) {
      const datasource = await DataSource.findOne({ _id: datasourceId, userId })
      if (!datasource) {
        throw new Error('æ•°æ®æºä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®')
      }
    }
    
    // åˆ›å»ºæ•°æ®é›†
    const dataset = new Dataset({
      ...request,
      userId,
      status: 'active', // ç¡®ä¿çŠ¶æ€ä¸ºactive
      // ä½¿ç”¨å‰ç«¯ä¼ é€’çš„å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¸´æ—¶å­—æ®µ
      fields: request.fields && request.fields.length > 0 ? request.fields : [{
        name: 'temp',
        displayName: 'ä¸´æ—¶å­—æ®µ',
        type: 'string',
        fieldType: 'dimension',
        isNullable: true
      }],
      metadata: {
        columns: request.fields ? request.fields.length : 1,
        recordCount: 0
      },
      permissions: [{
        userId,
        role: 'owner'
      }]
    })
    
    // ä¿å­˜æ•°æ®é›†ï¼ˆè·³è¿‡éªŒè¯ä»¥å…è®¸ä¸´æ—¶çŠ¶æ€ï¼‰
    const savedDataset = await dataset.save({ validateBeforeSave: false })
    
    // å¼‚æ­¥åˆ†æå­—æ®µç»“æ„
    setImmediate(() => {
      this.analyzeDatasetFields(savedDataset._id.toString()).catch(console.error)
    })
    
    return savedDataset.toJSON()
  }
  
  // æ›´æ–°æ•°æ®é›†
  static async updateDataset(userId: string, datasetId: string, request: UpdateDatasetRequest): Promise<DatasetType> {
    await connectDB()
    
    console.log('DatasetServiceæ¥æ”¶åˆ°çš„æ›´æ–°æ•°æ®:', JSON.stringify(request, null, 2))
    console.log('å­—æ®µå•ä½ä¿¡æ¯:', request.fields?.map(f => ({ name: f.name, fieldType: f.fieldType, unit: f.unit })))
    
    const dataset = await Dataset.findOne({ _id: datasetId })
    if (!dataset) {
      throw new Error('æ•°æ®é›†ä¸å­˜åœ¨')
    }
    
    if (!dataset.hasPermission(userId, 'editor')) {
      throw new Error('æ— æƒé™ç¼–è¾‘æ­¤æ•°æ®é›†')
    }
    
    console.log('å‡†å¤‡æ›´æ–°çš„æ•°æ®:', { ...request })
    
    const updatedDataset = await Dataset.findOneAndUpdate(
      { _id: datasetId },
      { ...request },
      { new: true }
    )
    
    if (!updatedDataset) {
      throw new Error('æ›´æ–°å¤±è´¥')
    }
    
    console.log('æ›´æ–°åçš„æ•°æ®é›†å­—æ®µ:', updatedDataset.fields?.map(f => ({ name: f.name, fieldType: f.fieldType, unit: f.unit })))
    
    // æ¸…é™¤ç›¸å…³ç¼“å­˜
    this.invalidateDatasetCache(datasetId)
    
    // æ³¨æ„ï¼šæ›´æ–°æ•°æ®é›†æ—¶ä¸é‡æ–°è§¦å‘å­—æ®µåˆ†æï¼Œä»¥ä¿ç•™ç”¨æˆ·è‡ªå®šä¹‰è®¾ç½®
    
    return updatedDataset.toJSON()
  }
  
  // è·å–æ•°æ®é›†
  static async getDataset(userId: string, datasetId: string): Promise<DatasetType> {
    const cacheKey = CacheKeys.dataset(datasetId)
    
    // å°è¯•ä»ç¼“å­˜è·å–
    const cached = datasetCache.get<DatasetType>(cacheKey)
    if (cached) {
      // ä»éœ€éªŒè¯æƒé™ï¼Œä½†é¿å…æ•°æ®åº“æŸ¥è¯¢
      return cached
    }
    
    return datasetCache.getOrSet(cacheKey, async () => {
      await connectDB()
      
      try {
        const dataset = await Dataset.findOne({ _id: datasetId })
        
        if (!dataset) {
          throw new Error('æ•°æ®é›†ä¸å­˜åœ¨')
        }
        
        if (!dataset.hasPermission(userId, 'viewer')) {
          throw new Error('æ— æƒé™è®¿é—®æ­¤æ•°æ®é›†')
        }
        
        // å…ˆè½¬æ¢ä¸ºJSONï¼Œé¿å…populateå¼•èµ·çš„åºåˆ—åŒ–é—®é¢˜
        const datasetJson = dataset.toJSON()
        
        // æ‰‹åŠ¨è·å–å…³è”çš„æ•°æ®æºä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if (datasetJson.tableConfig?.datasourceId) {
          try {
            const datasource = await DataSource.findById(datasetJson.tableConfig.datasourceId).select('name type')
            if (datasource) {
              datasetJson.tableConfig.datasourceId = {
                _id: datasource._id,
                name: datasource.name,
                type: datasource.type
              }
            }
          } catch (dsError) {
            console.warn('Failed to populate tableConfig datasource:', dsError)
          }
        }
        
        if (datasetJson.sqlConfig?.datasourceId) {
          try {
            const datasource = await DataSource.findById(datasetJson.sqlConfig.datasourceId).select('name type')
            if (datasource) {
              datasetJson.sqlConfig.datasourceId = {
                _id: datasource._id,
                name: datasource.name,
                type: datasource.type
              }
            }
          } catch (dsError) {
            console.warn('Failed to populate sqlConfig datasource:', dsError)
          }
        }
        
        return datasetJson
      } catch (error) {
        console.error('Error in getDataset:', error)
        throw error
      }
    }, {
      ttl: 10 * 60 * 1000, // 10 minutes
      tags: [`dataset:${datasetId}`, `user:${userId}`]
    })
  }
  
  // æœç´¢æ•°æ®é›†
  static async searchDatasets(userId: string, params: DatasetSearchParams) {
    try {
      await connectDB()
      
      const {
        keyword,
        category,
        tags,
        type,
        status = process.env.NODE_ENV === 'development' ? undefined : 'active', // å¼€å‘ç¯å¢ƒæ˜¾ç¤ºæ‰€æœ‰çŠ¶æ€
        sortBy = 'updatedAt',
        sortOrder = 'desc',
        page = 1,
        limit = 20
      } = params
    
    // æ„å»ºæŸ¥è¯¢æ¡ä»¶
    const query: any = {
      $or: [
        { userId },
        { 'permissions.userId': userId }
      ]
    }
    
    // åªåœ¨æŒ‡å®šstatusæ—¶æ‰æ·»åŠ çŠ¶æ€è¿‡æ»¤
    if (status) {
      query.status = status
    }
    
    if (keyword) {
      query.$text = { $search: keyword }
    }
    
    if (category) {
      query.category = category
    }
    
    if (tags && tags.length > 0) {
      query.tags = { $in: tags }
    }
    
    if (type) {
      query.type = type
    }
    
    // æ„å»ºæ’åº
    const sortObj: any = {}
    if (keyword && sortBy === 'relevance') {
      sortObj.score = { $meta: 'textScore' }
    } else {
      sortObj[sortBy] = sortOrder === 'desc' ? -1 : 1
    }
    
    // æ‰§è¡ŒæŸ¥è¯¢
    const [datasets, total] = await Promise.all([
      Dataset.find(query)
        .sort(sortObj)
        .skip((page - 1) * limit)
        .limit(limit)
        .populate('tableConfig.datasourceId', 'name type')
        .populate('sqlConfig.datasourceId', 'name type')
        .lean(),
      Dataset.countDocuments(query)
    ])
    
    // è·å–è¿‡æ»¤é€‰é¡¹çš„åŸºç¡€æŸ¥è¯¢æ¡ä»¶
    const baseFilterQuery = { 
      $or: [{ userId }, { 'permissions.userId': userId }]
    }
    
    // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åªæ˜¾ç¤ºactiveçŠ¶æ€çš„è¿‡æ»¤é€‰é¡¹
    if (process.env.NODE_ENV !== 'development') {
      (baseFilterQuery as any).status = 'active'
    }
    
    const [categories, allTags, types] = await Promise.all([
      Dataset.distinct('category', baseFilterQuery),
      Dataset.distinct('tags', baseFilterQuery),
      Dataset.distinct('type', baseFilterQuery)
    ])
    
      return {
        datasets,
        pagination: {
          page,
          limit,
          total,
          totalPages: Math.ceil(total / limit)
        },
        filters: {
          categories,
          tags: allTags.flat(),
          types
        }
      }
    } catch (error) {
      console.error('æœç´¢æ•°æ®é›†å¤±è´¥:', error)
      
      // åœ¨å¼€å‘ç¯å¢ƒè¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
      if (process.env.NODE_ENV === 'development') {
        return {
          datasets: [],
          pagination: {
            page: 1,
            limit: 20,
            total: 0,
            totalPages: 0
          },
          filters: {
            categories: [],
            tags: [],
            types: []
          }
        }
      }
      
      throw error
    }
  }
  
  // é¢„è§ˆæ•°æ®é›†
  static async previewDataset(userId: string, datasetId: string, limit: number = 100): Promise<DatasetPreview> {
    const cacheKey = CacheKeys.datasetPreview(datasetId, limit)
    
    return queryCache.getOrSet(cacheKey, async () => {
      const dataset = await this.getDataset(userId, datasetId)
      console.log('ğŸ” é¢„è§ˆæ•°æ®é›†:', dataset.name, 'ID:', datasetId)
      console.log('ğŸ” æ•°æ®é›†é…ç½®:', {
        type: dataset.type,
        tableConfig: dataset.tableConfig,
        sqlConfig: dataset.sqlConfig,
        fieldsCount: dataset.fields?.length
      })
      
      const startTime = Date.now()
      
      try {
        let query = ''
        let params: any[] = []
        
        if (dataset.type === 'table' && dataset.tableConfig) {
          const tableName = dataset.tableConfig.schema 
            ? `${dataset.tableConfig.schema}.${dataset.tableConfig.tableName}`
            : dataset.tableConfig.tableName
          query = `SELECT * FROM ${tableName} LIMIT ${limit}`
          params = []
        } else if (dataset.type === 'sql' && dataset.sqlConfig) {
          // åŒ…è£…ç”¨æˆ·SQLä»¥é™åˆ¶è¿”å›è¡Œæ•°
          query = `SELECT * FROM (${dataset.sqlConfig.sql}) AS subquery LIMIT ${limit}`
          params = []
        } else if (dataset.type === 'view' && dataset.viewConfig) {
          // åŸºäºçˆ¶æ•°æ®é›†æ„å»ºæŸ¥è¯¢
          const baseDataset = await this.getDataset(userId, dataset.viewConfig.baseDatasetId)
          // è¿™é‡Œéœ€è¦æ ¹æ®è¿‡æ»¤æ¡ä»¶æ„å»ºæŸ¥è¯¢ - ç®€åŒ–å®ç°
          query = `SELECT * FROM (${this.buildViewQuery(baseDataset, dataset.viewConfig)}) AS view_query LIMIT ${limit}`
          params = []
        } else {
          throw new Error('ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹')
        }
        
        // è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
        const datasourceId = dataset.tableConfig?.datasourceId || dataset.sqlConfig?.datasourceId
        if (!datasourceId) {
          throw new Error('ç¼ºå°‘æ•°æ®æºé…ç½®')
        }
        
        // è·å–åŒ…å«å¯†ç çš„æ•°æ®æºé…ç½®
        const datasourceRaw = await DataSource.findById(datasourceId).lean()
        if (!datasourceRaw) {
          throw new Error('æ•°æ®æºä¸å­˜åœ¨')
        }
        
        const datasourceWithPassword = await DataSource.findById(datasourceId).select('+config.password').lean()
        const datasource = {
          ...datasourceRaw,
          config: {
            ...datasourceRaw.config,
            password: datasourceWithPassword?.config?.password
          }
        } as any
        
        // æ‰§è¡ŒæŸ¥è¯¢
        console.log('ğŸ” æ‰§è¡Œé¢„è§ˆæŸ¥è¯¢:', query)
        console.log('ğŸ” æŸ¥è¯¢å‚æ•°:', params)
        
        const result = await executeQuery(datasource.config, query, params)
        console.log('ğŸ” æŸ¥è¯¢ç»“æœ:', {
          dataLength: result.data?.length,
          columnsLength: result.columns?.length,
          total: result.total
        })
        
        return {
          columns: dataset.fields,
          rows: result.data,
          totalCount: result.total,
          executionTime: Date.now() - startTime
        }
      } catch (error) {
        return {
          columns: dataset.fields,
          rows: [],
          totalCount: 0,
          executionTime: Date.now() - startTime,
          errors: [error instanceof Error ? error.message : 'æŸ¥è¯¢å¤±è´¥']
        }
      }
    }, {
      ttl: 5 * 60 * 1000, // 5 minutes
      tags: [`dataset:${datasetId}`, 'preview']
    })
  }
  
  // åˆ†ææ•°æ®é›†å­—æ®µï¼ˆè‡ªåŠ¨è¯†åˆ«åº¦é‡å’Œç»´åº¦ï¼‰
  static async analyzeDatasetFields(datasetId: string): Promise<void> {
    try {
      await connectDB()
      
      const dataset = await Dataset.findById(datasetId)
      if (!dataset) return
      
      // è·å–æ•°æ®æº
      let datasourceId: string = ''
      if (dataset.tableConfig) {
        datasourceId = dataset.tableConfig.datasourceId.toString()
      } else if (dataset.sqlConfig) {
        datasourceId = dataset.sqlConfig.datasourceId.toString()
      }
      
      if (!datasourceId) return
      
      // è·å–åŒ…å«å¯†ç çš„æ•°æ®æºé…ç½®
      const datasourceRaw = await DataSource.findById(datasourceId).lean()
      if (!datasourceRaw) return
      
      const datasourceWithPassword = await DataSource.findById(datasourceId).select('+config.password').lean()
      const datasource = {
        ...datasourceRaw,
        config: {
          ...datasourceRaw.config,
          password: datasourceWithPassword?.config?.password
        }
      } as any
      
      // æ„å»ºæŸ¥è¯¢è·å–å­—æ®µä¿¡æ¯å’Œæ ·æœ¬æ•°æ®
      let query = ''
      if (dataset.type === 'table' && dataset.tableConfig) {
        const tableName = dataset.tableConfig.schema 
          ? `${dataset.tableConfig.schema}.${dataset.tableConfig.tableName}`
          : dataset.tableConfig.tableName
        query = `SELECT * FROM ${tableName} LIMIT 1000`
      } else if (dataset.type === 'sql' && dataset.sqlConfig) {
        query = `SELECT * FROM (${dataset.sqlConfig.sql}) AS subquery LIMIT 1000`
      }
      
      if (!query) return
      
      const result = await executeQuery(datasource.config, query)
      if (!result.data || result.data.length === 0) return
      
      // åˆ†æå­—æ®µç±»å‹
      const analyzedFields = this.analyzeFieldTypes(result.columns, result.data)
      
      // ä¿ç•™ç°æœ‰å­—æ®µçš„ç”¨æˆ·è‡ªå®šä¹‰å±æ€§ï¼ˆå¦‚æ˜¾ç¤ºåç§°ï¼‰
      const existingFieldsMap = new Map(dataset.fields.map(f => [f.name, f]))
      
      const mergedFields = analyzedFields.map(analyzedField => {
        const existingField = existingFieldsMap.get(analyzedField.name)
        if (existingField) {
          // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„å±æ€§ï¼Œä½†æ›´æ–°åˆ†æå¾—å‡ºçš„ç±»å‹å’Œæ ·æœ¬å€¼
          const merged = {
            ...analyzedField,
            displayName: existingField.displayName || analyzedField.displayName, // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„æ˜¾ç¤ºåç§°
            description: existingField.description || analyzedField.description, // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„æè¿°
            fieldType: existingField.fieldType || analyzedField.fieldType, // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„å­—æ®µç±»å‹
            aggregationType: existingField.aggregationType || analyzedField.aggregationType, // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„èšåˆç±»å‹
            dimensionLevel: existingField.dimensionLevel || analyzedField.dimensionLevel, // ä¿ç•™ç”¨æˆ·è®¾ç½®çš„ç»´åº¦çº§åˆ«
            expression: existingField.expression, // ä¿ç•™è®¡ç®—å­—æ®µçš„è¡¨è¾¾å¼
            format: existingField.format, // ä¿ç•™æ ¼å¼è®¾ç½®
            hidden: existingField.hidden // ä¿ç•™éšè—çŠ¶æ€
          }
          return merged
        }
        return analyzedField
      })
      
      
      // è®¡ç®—æ•°æ®è´¨é‡
      const qualityIssues = this.analyzeDataQuality(mergedFields, result.data)
      const qualityScore = this.calculateQualityScore(qualityIssues)
      
      // æ›´æ–°æ•°æ®é›†
      await Dataset.findByIdAndUpdate(datasetId, {
        fields: mergedFields,
        metadata: {
          recordCount: result.total,
          lastRefreshed: new Date(),
          dataSize: JSON.stringify(result.data).length,
          columns: result.columns.length
        },
        qualityIssues,
        qualityScore,
        status: 'active'
      })
      
    } catch (error) {
      console.error('å­—æ®µåˆ†æå¤±è´¥:', error)
      // æ›´æ–°çŠ¶æ€ä¸ºé”™è¯¯
      await Dataset.findByIdAndUpdate(datasetId, {
        status: 'error',
        lastError: error instanceof Error ? error.message : 'å­—æ®µåˆ†æå¤±è´¥'
      })
    }
  }
  
  // åˆ†æå­—æ®µç±»å‹å’Œç‰¹å¾
  private static analyzeFieldTypes(columns: any[], sampleData: any[]): DatasetField[] {
    return columns.map(col => {
      const fieldName = col.name
      const values = sampleData.map(row => row[fieldName]).filter(v => v != null)
      
      if (values.length === 0) {
        return {
          name: fieldName,
          displayName: fieldName,
          type: 'string' as const,
          fieldType: 'dimension' as const,
          isNullable: true,
          sampleValues: []
        }
      }
      
      // åˆ†ææ•°æ®ç±»å‹
      const dataType = this.inferDataType(values)
      
      // åˆ¤æ–­æ˜¯åº¦é‡è¿˜æ˜¯ç»´åº¦
      const fieldType = this.inferFieldType(fieldName, dataType, values)
      
      // ç”Ÿæˆæ ·æœ¬å€¼
      const uniqueValues = [...new Set(values)]
      const sampleValues = uniqueValues.slice(0, 10)
      
      const field: DatasetField = {
        name: fieldName,
        displayName: this.generateDisplayName(fieldName),
        type: dataType,
        fieldType,
        isNullable: sampleData.some(row => row[fieldName] == null),
        sampleValues
      }
      
      // å¦‚æœæ˜¯åº¦é‡ï¼Œæ¨æ–­èšåˆç±»å‹
      if (fieldType === 'measure') {
        field.aggregationType = this.inferAggregationType(fieldName, values)
      }
      
      // å¦‚æœæ˜¯ç»´åº¦ï¼Œæ¨æ–­ç»´åº¦çº§åˆ«
      if (fieldType === 'dimension') {
        field.dimensionLevel = this.inferDimensionLevel(fieldName, dataType, values)
      }
      
      return field
    })
  }
  
  // æ¨æ–­æ•°æ®ç±»å‹
  private static inferDataType(values: any[]): 'string' | 'number' | 'date' | 'boolean' {
    const sample = values.slice(0, 100) // åªåˆ†æå‰100ä¸ªå€¼
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºå¸ƒå°”å€¼
    if (sample.every(v => typeof v === 'boolean' || v === 0 || v === 1 || v === '0' || v === '1' || v === 'true' || v === 'false')) {
      return 'boolean'
    }
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—
    const numericCount = sample.filter(v => !isNaN(Number(v)) && isFinite(Number(v))).length
    if (numericCount / sample.length > 0.8) {
      return 'number'
    }
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸ
    const dateCount = sample.filter(v => {
      const date = new Date(v)
      return !isNaN(date.getTime()) && typeof v === 'string' && v.length > 8
    }).length
    if (dateCount / sample.length > 0.8) {
      return 'date'
    }
    
    return 'string'
  }
  
  // æ¨æ–­å­—æ®µç±»å‹ï¼ˆåº¦é‡æˆ–ç»´åº¦ï¼‰
  private static inferFieldType(fieldName: string, dataType: string, values: any[]): 'dimension' | 'measure' {
    // å­—æ®µååŒ…å«åº¦é‡å…³é”®è¯
    const measureKeywords = ['count', 'sum', 'total', 'amount', 'price', 'cost', 'revenue', 'profit', 'qty', 'quantity', 'rate', 'percentage', 'æ•°é‡', 'é‡‘é¢', 'æ€»é¢', 'è´¹ç”¨', 'ä»·æ ¼', 'æ¯”ç‡']
    if (measureKeywords.some(keyword => fieldName.toLowerCase().includes(keyword))) {
      return 'measure'
    }
    
    // æ•°å­—ç±»å‹ä¸”å”¯ä¸€å€¼è¾ƒå¤šï¼Œå¯èƒ½æ˜¯åº¦é‡
    if (dataType === 'number') {
      const uniqueCount = new Set(values).size
      const totalCount = values.length
      
      // å¦‚æœå”¯ä¸€å€¼æ¯”ä¾‹å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯åº¦é‡
      if (uniqueCount / totalCount > 0.5) {
        return 'measure'
      }
    }
    
    // å…¶ä»–æƒ…å†µé»˜è®¤ä¸ºç»´åº¦
    return 'dimension'
  }
  
  // æ¨æ–­èšåˆç±»å‹
  private static inferAggregationType(fieldName: string, values: any[]): 'SUM' | 'AVG' | 'COUNT' | 'MAX' | 'MIN' {
    const name = fieldName.toLowerCase()
    
    if (name.includes('count') || name.includes('num') || name.includes('æ•°é‡')) {
      return 'COUNT'
    }
    
    if (name.includes('total') || name.includes('sum') || name.includes('amount') || name.includes('æ€»')) {
      return 'SUM'
    }
    
    if (name.includes('avg') || name.includes('average') || name.includes('rate') || name.includes('å¹³å‡')) {
      return 'AVG'
    }
    
    // é»˜è®¤ä½¿ç”¨SUM
    return 'SUM'
  }
  
  // æ¨æ–­ç»´åº¦çº§åˆ«
  private static inferDimensionLevel(fieldName: string, dataType: string, values: any[]): 'categorical' | 'ordinal' | 'temporal' {
    if (dataType === 'date') {
      return 'temporal'
    }
    
    const name = fieldName.toLowerCase()
    if (name.includes('date') || name.includes('time') || name.includes('å¹´') || name.includes('æœˆ') || name.includes('æ—¥')) {
      return 'temporal'
    }
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰åºåˆ†ç±»
    const ordinalKeywords = ['level', 'grade', 'rating', 'priority', 'rank', 'ç­‰çº§', 'çº§åˆ«', 'è¯„åˆ†']
    if (ordinalKeywords.some(keyword => name.includes(keyword))) {
      return 'ordinal'
    }
    
    return 'categorical'
  }
  
  // ç”Ÿæˆæ˜¾ç¤ºåç§°
  private static generateDisplayName(fieldName: string): string {
    // ç®€å•çš„é©¼å³°è½¬æ¢å’Œç¾åŒ–
    return fieldName
      .replace(/([A-Z])/g, ' $1')
      .replace(/[_-]/g, ' ')
      .replace(/\b\w/g, l => l.toUpperCase())
      .trim()
  }
  
  // åˆ†ææ•°æ®è´¨é‡
  private static analyzeDataQuality(fields: DatasetField[], sampleData: any[]): DataQualityIssue[] {
    const issues: DataQualityIssue[] = []
    const totalRecords = sampleData.length
    
    fields.forEach(field => {
      const fieldName = field.name
      const values = sampleData.map(row => row[fieldName])
      
      // æ£€æŸ¥ç¼ºå¤±å€¼
      const nullCount = values.filter(v => v == null || v === '').length
      if (nullCount > 0) {
        const percentage = (nullCount / totalRecords) * 100
        issues.push({
          type: 'missing_values',
          field: fieldName,
          count: nullCount,
          percentage,
          description: `å­—æ®µ ${field.displayName} æœ‰ ${nullCount} ä¸ªç¼ºå¤±å€¼`,
          severity: percentage > 50 ? 'high' : percentage > 20 ? 'medium' : 'low'
        })
      }
      
      // æ£€æŸ¥é‡å¤è®°å½•ï¼ˆä»…å¯¹ä¸»é”®å­—æ®µï¼‰
      if (field.isPrimaryKey) {
        const nonNullValues = values.filter(v => v != null && v !== '')
        const uniqueCount = new Set(nonNullValues).size
        const duplicateCount = nonNullValues.length - uniqueCount
        
        if (duplicateCount > 0) {
          const percentage = (duplicateCount / totalRecords) * 100
          issues.push({
            type: 'duplicate_records',
            field: fieldName,
            count: duplicateCount,
            percentage,
            description: `ä¸»é”®å­—æ®µ ${field.displayName} æœ‰ ${duplicateCount} ä¸ªé‡å¤å€¼`,
            severity: 'high'
          })
        }
      }
    })
    
    return issues
  }
  
  // è®¡ç®—è´¨é‡è¯„åˆ†
  private static calculateQualityScore(issues: DataQualityIssue[]): number {
    if (issues.length === 0) return 100
    
    let score = 100
    
    issues.forEach(issue => {
      let penalty = 0
      
      switch (issue.severity) {
        case 'high':
          penalty = Math.min(issue.percentage * 0.8, 30)
          break
        case 'medium':
          penalty = Math.min(issue.percentage * 0.5, 20)
          break
        case 'low':
          penalty = Math.min(issue.percentage * 0.2, 10)
          break
      }
      
      score -= penalty
    })
    
    return Math.max(0, Math.round(score))
  }
  
  // æ„å»ºè§†å›¾æŸ¥è¯¢ï¼ˆç®€åŒ–ç‰ˆï¼‰
  private static buildViewQuery(baseDataset: DatasetType, viewConfig: any): string {
    // è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®viewConfigçš„filterså’ŒcomputedFieldsæ„å»ºå¤æ‚æŸ¥è¯¢
    if (baseDataset.type === 'table' && baseDataset.tableConfig) {
      const tableName = baseDataset.tableConfig.schema 
        ? `${baseDataset.tableConfig.schema}.${baseDataset.tableConfig.tableName}`
        : baseDataset.tableConfig.tableName
      return `SELECT * FROM ${tableName}`
    } else if (baseDataset.type === 'sql' && baseDataset.sqlConfig) {
      return baseDataset.sqlConfig.sql
    }
    
    throw new Error('æ— æ³•æ„å»ºè§†å›¾æŸ¥è¯¢')
  }
  
  // æŸ¥è¯¢æ•°æ®é›†æ•°æ®ï¼ˆæ”¯æŒåº¦é‡ã€ç»´åº¦å’Œç­›é€‰å™¨ï¼‰
  static async queryDataset(userId: string, datasetId: string, options: {
    measures: string[]
    dimensions: string[]
    filters: any[]
    limit: number
  }) {
    // åˆ›å»ºæŸ¥è¯¢å‚æ•°çš„å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
    const queryHash = this.createQueryHash(options)
    const cacheKey = CacheKeys.datasetQuery(datasetId, queryHash)
    
    return queryCache.getOrSet(cacheKey, async () => {
      const dataset = await this.getDataset(userId, datasetId)
      const startTime = Date.now()
      
      try {
        let query = ''
        let params: any[] = []
        
        // æ„å»ºSELECTå­å¥
        const selectFields = []
        
        // æ·»åŠ ç»´åº¦å­—æ®µ
        options.dimensions.forEach(dim => {
          selectFields.push(dim)
        })
        
        // æ·»åŠ åº¦é‡å­—æ®µï¼ˆå¸¦èšåˆå‡½æ•°ï¼‰
        options.measures.forEach(measure => {
          const field = dataset.fields.find(f => f.name === measure)
          const aggType = field?.aggregationType || 'SUM'
          selectFields.push(`${aggType}(${measure}) as ${measure}`)
        })
        
        // å¦‚æœæ²¡æœ‰æŒ‡å®šå­—æ®µï¼Œé€‰æ‹©æ‰€æœ‰å­—æ®µ
        if (selectFields.length === 0) {
          selectFields.push('*')
        }
        
        const selectClause = selectFields.join(', ')
        
        // æ„å»ºFROMå­å¥
        let fromClause = ''
        if (dataset.type === 'table' && dataset.tableConfig) {
          fromClause = dataset.tableConfig.schema 
            ? `${dataset.tableConfig.schema}.${dataset.tableConfig.tableName}`
            : dataset.tableConfig.tableName
        } else if (dataset.type === 'sql' && dataset.sqlConfig) {
          fromClause = `(${dataset.sqlConfig.sql}) AS subquery`
        } else if (dataset.type === 'view' && dataset.viewConfig) {
          const baseDataset = await this.getDataset(userId, dataset.viewConfig.baseDatasetId)
          fromClause = `(${this.buildViewQuery(baseDataset, dataset.viewConfig)}) AS view_query`
        } else {
          throw new Error('ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹')
        }
        
        // æ„å»ºWHEREå­å¥ï¼ˆå¤„ç†ç­›é€‰å™¨ï¼‰
        const whereConditions = []
        options.filters.forEach((filter, index) => {
          if (filter.field && filter.operator && filter.value !== undefined) {
            switch (filter.operator) {
              case 'equals':
                whereConditions.push(`${filter.field} = ?`)
                params.push(filter.value)
                break
              case 'not_equals':
                whereConditions.push(`${filter.field} != ?`)
                params.push(filter.value)
                break
              case 'greater_than':
                whereConditions.push(`${filter.field} > ?`)
                params.push(filter.value)
                break
              case 'less_than':
                whereConditions.push(`${filter.field} < ?`)
                params.push(filter.value)
                break
              case 'contains':
                whereConditions.push(`${filter.field} LIKE ?`)
                params.push(`%${filter.value}%`)
                break
              case 'in':
                if (Array.isArray(filter.value) && filter.value.length > 0) {
                  const placeholders = filter.value.map(() => '?').join(', ')
                  whereConditions.push(`${filter.field} IN (${placeholders})`)
                  params.push(...filter.value)
                }
                break
            }
          }
        })
        
        const whereClause = whereConditions.length > 0 ? `WHERE ${whereConditions.join(' AND ')}` : ''
        
        // æ„å»ºGROUP BYå­å¥ï¼ˆå¦‚æœæœ‰ç»´åº¦å­—æ®µï¼‰
        const groupByClause = options.dimensions.length > 0 ? `GROUP BY ${options.dimensions.join(', ')}` : ''
        
        // æ„å»ºORDER BYå­å¥
        let orderByClause = ''
        if (options.measures.length > 0) {
          orderByClause = `ORDER BY ${options.measures[0]} DESC`
        } else if (options.dimensions.length > 0) {
          orderByClause = `ORDER BY ${options.dimensions[0]}`
        }
        
        // æ„å»ºå®Œæ•´æŸ¥è¯¢
        query = [
          `SELECT ${selectClause}`,
          `FROM ${fromClause}`,
          whereClause,
          groupByClause,
          orderByClause,
          `LIMIT ?`
        ].filter(Boolean).join(' ')
        
        params.push(options.limit)
        
        // è·å–æ•°æ®æºè¿æ¥ä¿¡æ¯
        const datasourceId = dataset.tableConfig?.datasourceId || dataset.sqlConfig?.datasourceId
        if (!datasourceId) {
          throw new Error('ç¼ºå°‘æ•°æ®æºé…ç½®')
        }
        
        // è·å–åŒ…å«å¯†ç çš„æ•°æ®æºé…ç½®
        const datasourceRaw = await DataSource.findById(datasourceId).lean()
        if (!datasourceRaw) {
          throw new Error('æ•°æ®æºä¸å­˜åœ¨')
        }
        
        const datasourceWithPassword = await DataSource.findById(datasourceId).select('+config.password').lean()
        const datasource = {
          ...datasourceRaw,
          config: {
            ...datasourceRaw.config,
            password: datasourceWithPassword?.config?.password
          }
        } as any
        
        // æ‰§è¡ŒæŸ¥è¯¢
        const result = await executeQuery(datasource.config, query, params)
        
        return {
          data: result.data,
          columns: result.columns || dataset.fields,
          total: result.total || result.data?.length || 0,
          executionTime: Date.now() - startTime
        }
      } catch (error) {
        console.error('æ•°æ®é›†æŸ¥è¯¢å¤±è´¥:', error)
        return {
          data: [],
          columns: dataset.fields,
          total: 0,
          executionTime: Date.now() - startTime,
          error: error instanceof Error ? error.message : 'æŸ¥è¯¢å¤±è´¥'
        }
      }
    }, {
      ttl: 3 * 60 * 1000, // 3 minutes
      tags: [`dataset:${datasetId}`, 'query']
    })
  }

  // åˆ é™¤æ•°æ®é›†
  static async deleteDataset(userId: string, datasetId: string): Promise<void> {
    await connectDB()
    
    const dataset = await Dataset.findOne({ _id: datasetId })
    if (!dataset) {
      throw new Error('æ•°æ®é›†ä¸å­˜åœ¨')
    }
    
    if (!dataset.hasPermission(userId, 'owner')) {
      throw new Error('æ— æƒé™åˆ é™¤æ­¤æ•°æ®é›†')
    }
    
    await Dataset.findByIdAndDelete(datasetId)
    
    // æ¸…é™¤ç›¸å…³ç¼“å­˜
    this.invalidateDatasetCache(datasetId)
  }

  // ç¼“å­˜è¾…åŠ©æ–¹æ³•

  /**
   * åˆ›å»ºæŸ¥è¯¢å‚æ•°çš„å“ˆå¸Œå€¼
   */
  private static createQueryHash(options: {
    measures: string[]
    dimensions: string[]
    filters: any[]
    limit: number
  }): string {
    const hashObject = {
      measures: options.measures.sort(),
      dimensions: options.dimensions.sort(),
      filters: options.filters.map(f => ({
        field: f.field,
        operator: f.operator,
        value: f.value
      })).sort((a, b) => a.field.localeCompare(b.field)),
      limit: options.limit
    }
    
    // ç®€å•çš„å“ˆå¸Œå®ç°
    return Buffer.from(JSON.stringify(hashObject)).toString('base64')
  }

  /**
   * æ¸…é™¤æ•°æ®é›†ç›¸å…³çš„æ‰€æœ‰ç¼“å­˜
   */
  private static invalidateDatasetCache(datasetId: string): void {
    // æ¸…é™¤æ•°æ®é›†æœ¬èº«çš„ç¼“å­˜
    datasetCache.removeByTags([`dataset:${datasetId}`])
    
    // æ¸…é™¤æŸ¥è¯¢ç¼“å­˜
    queryCache.removeByTags([`dataset:${datasetId}`])
    
    console.log(`æ¸…é™¤æ•°æ®é›† ${datasetId} ç›¸å…³ç¼“å­˜`)
  }

  /**
   * è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
   */
  static getCacheStats() {
    return {
      datasetCache: datasetCache.getStats(),
      queryCache: queryCache.getStats()
    }
  }

  /**
   * æ‰‹åŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜
   */
  static cleanupCache() {
    const datasetCleaned = datasetCache.cleanup()
    const queryCleaned = queryCache.cleanup()
    
    return {
      datasetCleaned,
      queryCleaned,
      total: datasetCleaned + queryCleaned
    }
  }

  // æ£€æŸ¥æ•°æ®é›†æƒé™ï¼ˆé™æ€æ–¹æ³•ï¼Œä¸ä¾èµ–Mongooseå®ä¾‹ï¼‰
  private static checkDatasetPermission(dataset: any, userId: string, requiredRole: 'viewer' | 'editor' | 'owner' = 'viewer'): boolean {
    // æ•°æ®é›†æ‹¥æœ‰è€…æ‹¥æœ‰æ‰€æœ‰æƒé™
    if (dataset.userId?.toString() === userId || dataset.userId === userId) {
      return true
    }
    
    // æ£€æŸ¥æƒé™åˆ—è¡¨
    if (!dataset.permissions || !Array.isArray(dataset.permissions)) {
      return false
    }
    
    const permission = dataset.permissions.find((p: any) => {
      const permissionUserId = p.userId?.toString() || p.userId
      return permissionUserId === userId
    })
    
    if (!permission) return false
    
    // æƒé™çº§åˆ«æ£€æŸ¥
    const roleLevel = { viewer: 1, editor: 2, owner: 3 }
    return roleLevel[permission.role] >= roleLevel[requiredRole]
  }

  // æ‰§è¡Œè‡ªå®šä¹‰SQLæŸ¥è¯¢
  static async executeCustomSQL(userId: string, datasetId: string, sql: string): Promise<{
    rows: any[]
    columns: Array<{ name: string; displayName?: string }>
    executionTime: number
  }> {
    const startTime = Date.now()
    
    try {
      // è·å–æ•°æ®é›†ä¿¡æ¯
      const dataset = await this.getDataset(userId, datasetId)
      if (!dataset) {
        throw new Error('æ•°æ®é›†ä¸å­˜åœ¨')
      }

      // æ£€æŸ¥ç”¨æˆ·æƒé™ï¼ˆæ‰‹åŠ¨å®ç°ï¼Œé¿å…ä¾èµ–Mongooseå®ä¾‹æ–¹æ³•ï¼‰
      const hasPermission = this.checkDatasetPermission(dataset, userId, 'viewer')
      if (!hasPermission) {
        throw new Error('æ²¡æœ‰è®¿é—®æƒé™')
      }

      // è·å–æ•°æ®æºä¿¡æ¯
      let datasourceId: any
      if (dataset.type === 'table' && dataset.tableConfig) {
        datasourceId = dataset.tableConfig.datasourceId
      } else if (dataset.type === 'sql' && dataset.sqlConfig) {
        datasourceId = dataset.sqlConfig.datasourceId
      } else if (dataset.type === 'view' && dataset.viewConfig) {
        // è§†å›¾ç±»å‹éœ€è¦è·å–åŸºç¡€æ•°æ®é›†çš„æ•°æ®æº
        const baseDataset = await this.getDataset(userId, dataset.viewConfig.baseDatasetId)
        if (baseDataset?.type === 'table' && baseDataset.tableConfig) {
          datasourceId = baseDataset.tableConfig.datasourceId
        } else if (baseDataset?.type === 'sql' && baseDataset.sqlConfig) {
          datasourceId = baseDataset.sqlConfig.datasourceId
        }
      }
      
      if (!datasourceId) {
        throw new Error('æ•°æ®æºé…ç½®ç¼ºå¤±')
      }

      // è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥datasourceIdçš„ç±»å‹å’Œå€¼
      console.log(`ğŸ” datasourceId ç±»å‹: ${typeof datasourceId}, å€¼:`, datasourceId)
      console.log(`ğŸ” datasourceId å­—ç¬¦ä¸²è¡¨ç¤º: ${JSON.stringify(datasourceId)}`)

      console.log(`ğŸ” æ‰§è¡Œè‡ªå®šä¹‰SQL - æ•°æ®é›†: ${dataset.displayName}, SQL: ${sql.substring(0, 100)}...`)

      // è·å–æ•°æ®æºé…ç½®ä¿¡æ¯ï¼ˆéœ€è¦åŒ…å«å¯†ç ï¼‰
      let datasourceIdString: string
      if (typeof datasourceId === 'string') {
        datasourceIdString = datasourceId
      } else if (datasourceId && typeof datasourceId === 'object') {
        // å¦‚æœæ˜¯å¯¹è±¡ï¼Œå°è¯•è·å– _id æˆ– id å­—æ®µ
        datasourceIdString = datasourceId._id || datasourceId.id || datasourceId.toString()
      } else {
        datasourceIdString = String(datasourceId)
      }

      console.log(`ğŸ” æå–çš„datasourceIdString: ${datasourceIdString}`)
      
      const DataSource = (await import('@/models/DataSource')).DataSource
      const datasource = await DataSource.findById(datasourceIdString).select('+config.password').lean()
      
      if (!datasource) {
        throw new Error('æ•°æ®æºä¸å­˜åœ¨')
      }

      // æ„å»ºæ•°æ®æºé…ç½®å¯¹è±¡
      const datasourceConfig = {
        host: datasource.config.host,
        port: datasource.config.port,
        database: datasource.config.database,
        username: datasource.config.username,
        password: datasource.config.password
      }

      // æ‰§è¡ŒSQLæŸ¥è¯¢ï¼Œä½¿ç”¨config, sql, paramsçš„è°ƒç”¨æ–¹å¼
      const result = await executeQuery(datasourceConfig, sql, [])
      
      const executionTime = Date.now() - startTime
      
      console.log(`âœ… SQLæ‰§è¡Œå®Œæˆ - è€—æ—¶: ${executionTime}ms, è¿”å›è¡Œæ•°: ${result.data?.length || 0}`)

      return {
        rows: result.data || [],
        columns: result.columns || [],
        executionTime
      }

    } catch (error) {
      const executionTime = Date.now() - startTime
      console.error(`âŒ SQLæ‰§è¡Œå¤±è´¥ - è€—æ—¶: ${executionTime}ms, é”™è¯¯:`, error)
      throw error
    }
  }
}